{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d45b7d-0ffe-42e1-bc8b-b3d277dc0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics + plotting\n",
    "import os, sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.dpi\"] = 250\n",
    "plt.rcParams[\"font.family\"] = \"sans serif\"\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# custom\n",
    "PROJECT_PATH = '/'.join(os.getcwd().split('/')[:-1])\n",
    "sys.path.insert(1, PROJECT_PATH)\n",
    "\n",
    "from utils import (\n",
    "    data_utils, \n",
    "    eval_utils, \n",
    "    plotting_utils, \n",
    "    train_test_utils\n",
    ")\n",
    "\n",
    "from models import (\n",
    "    esm_transfer_learning,\n",
    "    gnn\n",
    ")\n",
    "\n",
    "import importlib\n",
    "data_utils = importlib.reload(data_utils)\n",
    "eval_utils = importlib.reload(eval_utils)\n",
    "train_test_utils = importlib.reload(train_test_utils)\n",
    "esm_transfer_learning = importlib.reload(esm_transfer_learning)\n",
    "gnn = importlib.reload(gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e7d62c-7f33-49a8-a711-9fedf872a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching FHL1 data ... Done\n",
      "Fetching ACTC1 data ... Done\n",
      "Fetching ACTN2 data ... Done\n",
      "Fetching CSRP3 data ... Done\n",
      "Fetching MYBPC3 data ... Done\n",
      "Fetching MYH6 data ... Done\n",
      "Fetching MYH7 data ... Done\n",
      "Fetching MYL2 data ... Done\n",
      "Fetching MYL3 data ... Done\n",
      "Fetching MYOZ2 data ... Done\n",
      "Fetching LDB3 data ... Done\n",
      "Fetching TCAP data ... Done\n",
      "Fetching TNNC1 data ... Done\n",
      "Fetching TNNI3 data ... Done\n",
      "Fetching TNNT2 data ... Done\n",
      "Fetching TPM1 data ... Done\n",
      "Fetching TRIM63 data ... Done\n",
      "Fetching PLN data ... Done\n",
      "Fetching JPH2 data ... Done\n",
      "Fetching FLNC data ... Done\n",
      "Fetching ALPK3 data ... Done\n",
      "Fetching LMNA data ... Done\n",
      "Fetching NEXN data ... Done\n",
      "Fetching VCL data ... Done\n",
      "Fetching MYOM2 data ... Done\n",
      "Fetching CASQ2 data ... Done\n",
      "Fetching CAV3 data ... Done\n",
      "Fetching MYLK2 data ... Done\n",
      "Fetching CRYAB data ... Done\n",
      "Combining tables ... Done\n",
      "Integrating with phenotypes data ...Done\n",
      "Processing PPI graph ...Done\n"
     ]
    }
   ],
   "source": [
    "data = data_utils.load_variation_dataset(\"../data/data/\", \n",
    "                                         \"../gene_list.txt\", \n",
    "                                         [\"esm-tokens\", \"indicators\"], \n",
    "                                         \"../data/phenotypes_hcm_only.parquet\",\n",
    "                                         predict=[\"hcm\"], \n",
    "                                         low_memory=True,\n",
    "                                         embeddings_file='esm2m_embeddings.npy',\n",
    "                                         ppi_graph_path='../ppi_networks/string_interactions_short.tsv')\n",
    "\n",
    "train_dataset, test_dataset = data.train_test_split(balance_on=['hcm','ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e199a344-fd5f-49c2-965b-176206e90185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing for fine-tuning:\n",
      "- Freezing layer embed_tokens.weight\n",
      "- Freezing layer layers.0.self_attn.k_proj.weight\n",
      "- Freezing layer layers.0.self_attn.k_proj.bias\n",
      "- Freezing layer layers.0.self_attn.v_proj.weight\n",
      "- Freezing layer layers.0.self_attn.v_proj.bias\n",
      "- Freezing layer layers.0.self_attn.q_proj.weight\n",
      "- Freezing layer layers.0.self_attn.q_proj.bias\n",
      "- Freezing layer layers.0.self_attn.out_proj.weight\n",
      "- Freezing layer layers.0.self_attn.out_proj.bias\n",
      "- Freezing layer layers.0.self_attn_layer_norm.weight\n",
      "- Freezing layer layers.0.self_attn_layer_norm.bias\n",
      "- Freezing layer layers.0.fc1.weight\n",
      "- Freezing layer layers.0.fc1.bias\n",
      "- Freezing layer layers.0.fc2.weight\n",
      "- Freezing layer layers.0.fc2.bias\n",
      "- Freezing layer layers.0.final_layer_norm.weight\n",
      "- Freezing layer layers.0.final_layer_norm.bias\n",
      "- Freezing layer layers.1.self_attn.k_proj.weight\n",
      "- Freezing layer layers.1.self_attn.k_proj.bias\n",
      "- Freezing layer layers.1.self_attn.v_proj.weight\n",
      "- Freezing layer layers.1.self_attn.v_proj.bias\n",
      "- Freezing layer layers.1.self_attn.q_proj.weight\n",
      "- Freezing layer layers.1.self_attn.q_proj.bias\n",
      "- Freezing layer layers.1.self_attn.out_proj.weight\n",
      "- Freezing layer layers.1.self_attn.out_proj.bias\n",
      "- Freezing layer layers.1.self_attn_layer_norm.weight\n",
      "- Freezing layer layers.1.self_attn_layer_norm.bias\n",
      "- Freezing layer layers.1.fc1.weight\n",
      "- Freezing layer layers.1.fc1.bias\n",
      "- Freezing layer layers.1.fc2.weight\n",
      "- Freezing layer layers.1.fc2.bias\n",
      "- Freezing layer layers.1.final_layer_norm.weight\n",
      "- Freezing layer layers.1.final_layer_norm.bias\n",
      "- Freezing layer layers.2.self_attn.k_proj.weight\n",
      "- Freezing layer layers.2.self_attn.k_proj.bias\n",
      "- Freezing layer layers.2.self_attn.v_proj.weight\n",
      "- Freezing layer layers.2.self_attn.v_proj.bias\n",
      "- Freezing layer layers.2.self_attn.q_proj.weight\n",
      "- Freezing layer layers.2.self_attn.q_proj.bias\n",
      "- Freezing layer layers.2.self_attn.out_proj.weight\n",
      "- Freezing layer layers.2.self_attn.out_proj.bias\n",
      "- Freezing layer layers.2.self_attn_layer_norm.weight\n",
      "- Freezing layer layers.2.self_attn_layer_norm.bias\n",
      "- Freezing layer layers.2.fc1.weight\n",
      "- Freezing layer layers.2.fc1.bias\n",
      "- Freezing layer layers.2.fc2.weight\n",
      "- Freezing layer layers.2.fc2.bias\n",
      "- Freezing layer layers.2.final_layer_norm.weight\n",
      "- Freezing layer layers.2.final_layer_norm.bias\n",
      "- Freezing layer layers.3.self_attn.k_proj.weight\n",
      "- Freezing layer layers.3.self_attn.k_proj.bias\n",
      "- Freezing layer layers.3.self_attn.v_proj.weight\n",
      "- Freezing layer layers.3.self_attn.v_proj.bias\n",
      "- Freezing layer layers.3.self_attn.q_proj.weight\n",
      "- Freezing layer layers.3.self_attn.q_proj.bias\n",
      "- Freezing layer layers.3.self_attn.out_proj.weight\n",
      "- Freezing layer layers.3.self_attn.out_proj.bias\n",
      "- Freezing layer layers.3.self_attn_layer_norm.weight\n",
      "- Freezing layer layers.3.self_attn_layer_norm.bias\n",
      "- Freezing layer layers.3.fc1.weight\n",
      "- Freezing layer layers.3.fc1.bias\n",
      "- Freezing layer layers.3.fc2.weight\n",
      "- Freezing layer layers.3.fc2.bias\n",
      "- Freezing layer layers.3.final_layer_norm.weight\n",
      "- Freezing layer layers.3.final_layer_norm.bias\n",
      "- Freezing layer layers.4.self_attn.k_proj.weight\n",
      "- Freezing layer layers.4.self_attn.k_proj.bias\n",
      "- Freezing layer layers.4.self_attn.v_proj.weight\n",
      "- Freezing layer layers.4.self_attn.v_proj.bias\n",
      "- Freezing layer layers.4.self_attn.q_proj.weight\n",
      "- Freezing layer layers.4.self_attn.q_proj.bias\n",
      "- Freezing layer layers.4.self_attn.out_proj.weight\n",
      "- Freezing layer layers.4.self_attn.out_proj.bias\n",
      "- Freezing layer layers.4.self_attn_layer_norm.weight\n",
      "- Freezing layer layers.4.self_attn_layer_norm.bias\n",
      "- Freezing layer layers.4.fc1.weight\n",
      "- Freezing layer layers.4.fc1.bias\n",
      "- Freezing layer layers.4.fc2.weight\n",
      "- Freezing layer layers.4.fc2.bias\n",
      "- Freezing layer layers.4.final_layer_norm.weight\n",
      "- Freezing layer layers.4.final_layer_norm.bias\n",
      "+ Not freezing layer layers.5.self_attn.k_proj.weight\n",
      "+ Not freezing layer layers.5.self_attn.k_proj.bias\n",
      "+ Not freezing layer layers.5.self_attn.v_proj.weight\n",
      "+ Not freezing layer layers.5.self_attn.v_proj.bias\n",
      "+ Not freezing layer layers.5.self_attn.q_proj.weight\n",
      "+ Not freezing layer layers.5.self_attn.q_proj.bias\n",
      "+ Not freezing layer layers.5.self_attn.out_proj.weight\n",
      "+ Not freezing layer layers.5.self_attn.out_proj.bias\n",
      "+ Not freezing layer layers.5.self_attn_layer_norm.weight\n",
      "+ Not freezing layer layers.5.self_attn_layer_norm.bias\n",
      "+ Not freezing layer layers.5.fc1.weight\n",
      "+ Not freezing layer layers.5.fc1.bias\n",
      "+ Not freezing layer layers.5.fc2.weight\n",
      "+ Not freezing layer layers.5.fc2.bias\n",
      "+ Not freezing layer layers.5.final_layer_norm.weight\n",
      "+ Not freezing layer layers.5.final_layer_norm.bias\n",
      "- Freezing layer contact_head.regression.weight\n",
      "- Freezing layer contact_head.regression.bias\n",
      "- Freezing layer emb_layer_norm_after.weight\n",
      "- Freezing layer emb_layer_norm_after.bias\n",
      "- Freezing layer lm_head.bias\n",
      "- Freezing layer lm_head.dense.weight\n",
      "- Freezing layer lm_head.dense.bias\n",
      "- Freezing layer lm_head.layer_norm.weight\n",
      "- Freezing layer lm_head.layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = batch_size,\n",
    "    sampler = WeightedRandomSampler(train_dataset.weights('hcm',flatten_factor=1), num_samples = len(train_dataset)),\n",
    "    num_workers=14\n",
    ")\n",
    "    \n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    # sampler = WeightedRandomSampler(test_dataset.weights('hcm',flatten_factor=1), num_samples = 20),\n",
    "    num_workers=14\n",
    ")\n",
    "\n",
    "gat_params = {\n",
    "    'in_dim': 0,\n",
    "    'embed_dim': 256,\n",
    "    'n_heads': 4,\n",
    "    'n_nodes': 0,\n",
    "    'mlp_hidden_dims': [128],\n",
    "    'mlp_actn': 'gelu'\n",
    "}\n",
    "\n",
    "gcn_params = {\n",
    "    'in_dim': 0,\n",
    "    'embed_dim': 256,\n",
    "    'n_nodes': 0,\n",
    "    'mlp_hidden_dims': [256],\n",
    "    'mlp_actn': 'leakyrelu'\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'in_dim': 0,\n",
    "    'hidden_dims': [512],\n",
    "    'out_dim':0,\n",
    "    'actn': 'gelu'\n",
    "}\n",
    "\n",
    "model = esm_transfer_learning.ESMTransferLearner(\n",
    "    esm_model_name=\"esm2_t6_8M_UR50D\",\n",
    "    agg_emb_method=\"weighted_average\", \n",
    "    predict_method=\"gat\",\n",
    "    n_genes=29,\n",
    "    predictor_params=gat_params,\n",
    "    add_residue_features=data.get_feature_dimensions(['indicators']),\n",
    "    edge_index=data.edge_index.to('cuda'),\n",
    "    first_finetune_layer=-1,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3ca5c8-beb3-4ed8-aebe-45fa1a07f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 22.17 GiB total capacity; 20.08 GiB already allocated; 424.81 MiB free; 20.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_test_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_model_to\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/results/esm2s_wavg_gat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43msave_metrics_to\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mesm2s_wavg_gat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/n/groups/marks/databases/ukbiobank/users/ralphest/mit6.8610-project/utils/train_test_utils.py:137\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, save_model_to, save_metrics_to, log_every, lr, n_epochs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 137\u001b[0m     train_metrics, epoch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     eval_utils\u001b[38;5;241m.\u001b[39mprint_metrics(train_metrics)\n",
      "File \u001b[0;32m/n/groups/marks/databases/ukbiobank/users/ralphest/mit6.8610-project/utils/train_test_utils.py:63\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, loss_fn, device, log_every)\u001b[0m\n\u001b[1;32m     60\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# compute prediction and loss\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, labels)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/n/groups/marks/databases/ukbiobank/users/ralphest/mit6.8610-project/models/esm_transfer_learning.py:88\u001b[0m, in \u001b[0;36mESMTransferLearner.forward\u001b[0;34m(self, batch_dict)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_dict):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_finetune_layer:\n\u001b[0;32m---> 88\u001b[0m         batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune_esm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     protein_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_embeddings(batch_dict)\n\u001b[1;32m     90\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(protein_embeddings)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/n/groups/marks/databases/ukbiobank/users/ralphest/mit6.8610-project/models/esm_transfer_learning.py:282\u001b[0m, in \u001b[0;36mFinetuneESM.forward\u001b[0;34m(self, batch_dict)\u001b[0m\n\u001b[1;32m    280\u001b[0m residue_embeddings \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gene, toks \u001b[38;5;129;01min\u001b[39;00m batch_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mesm-tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 282\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     residue_embeddings[gene] \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers]\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m residue_embeddings\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/esm/model/esm2.py:112\u001b[0m, in \u001b[0;36mESM2.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    109\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 112\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[1;32m    118\u001b[0m         hidden_representations[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/esm/modules.py:125\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    123\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(x)\n\u001b[0;32m--> 125\u001b[0m x, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    136\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/esm/multihead_attention.py:379\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m before_softmax:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_weights, v\n\u001b[0;32m--> 379\u001b[0m attn_weights_float \u001b[38;5;241m=\u001b[39m \u001b[43mutils_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights_float\u001b[38;5;241m.\u001b[39mtype_as(attn_weights)\n\u001b[1;32m    381\u001b[0m attn_probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    382\u001b[0m     attn_weights_float\u001b[38;5;241m.\u001b[39mtype_as(attn_weights),\n\u001b[1;32m    383\u001b[0m     p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m    384\u001b[0m     training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m    385\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/esm/multihead_attention.py:22\u001b[0m, in \u001b[0;36mutils_softmax\u001b[0;34m(x, dim, onnx_trace)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msoftmax(x\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:1836\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1836\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.28 GiB (GPU 0; 22.17 GiB total capacity; 20.08 GiB already allocated; 424.81 MiB free; 20.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_test_utils.train(model, \n",
    "      train_loader,\n",
    "      test_loader,\n",
    "      save_model_to = '../data/results/esm2s_wavg_gat',\n",
    "      save_metrics_to = 'esm2s_wavg_gat',\n",
    "      log_every = 10,\n",
    "      lr=1e-3, \n",
    "      n_epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381e92d-459a-42f3-8827-b1941113703f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
