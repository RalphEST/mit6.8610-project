{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ef819a-7f01-4457-a039-01fb7cf6484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch import cuda\n",
    "import networkx as nx\n",
    "\n",
    "from utils import (\n",
    "    data_utils, \n",
    "    eval_utils, \n",
    "    plotting_utils, \n",
    "    train_test_utils\n",
    ")\n",
    "\n",
    "from models import (\n",
    "    GAT\n",
    ")\n",
    "\n",
    "# print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70fb7d3b-d159-41b2-aebe-642b5472d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "data_utils = importlib.reload(data_utils)\n",
    "eval_utils = importlib.reload(eval_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd830c9-4a19-4b9c-828e-4568f216917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"this is running in cpu!\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ca515-9edd-4d18-882f-08dcb408acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../gene_list.txt\", 'r') as file:\n",
    "    gene_list = [x.strip() for x in file.readlines()]\n",
    "\n",
    "train_dataset, test_dataset = data_utils.load_variation_dataset(data_dir='../data/data',\n",
    "                                                                gene_list=gene_list,\n",
    "                                                                data_types=['seq-var-matrix'], \n",
    "                                                                phenotypes_path=\"../data/131338.parquet\",\n",
    "                                                                keep_genes_separate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc2a323-949a-4fcb-b609-037d8f7eb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertPPIDataframeToNX(df):\n",
    "    Graphtype = nx.Graph()\n",
    "    G = nx.from_pandas_edgelist(df, source='#node1',target='node2',edge_attr=None, create_using=Graphtype)\n",
    "    # print(G.is_directed())\n",
    "    G = from_networkx(G)\n",
    "    return G\n",
    "\n",
    "df = pd.read_csv(\"ppi_networks/ppi2.tsv\",delimiter='\\t')\n",
    "G = convertPPIDataframeToNX(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5158347-dbcc-4bd9-b4e4-814a8cc7cc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, loss_fn, graph, log_every=10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_labels, all_preds = [],[]\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # move batch dictionary to device\n",
    "        data_utils.batch_dict_to_device(batch, device)\n",
    "        labels, features = batch['labels'], batch['embeddings'] #changed to batch['embeddings'] from  batch['seq-var-matrix']\n",
    "        \n",
    "        # compute prediction and loss\n",
    "        preds = model(features,graph.edge_index)\n",
    "        loss = loss_fn(preds, labels)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # tracking\n",
    "        total_loss += loss.item()\n",
    "        all_labels.append(labels.cpu())\n",
    "        all_preds.append(preds.flatten().detach().cpu())\n",
    "        \n",
    "        # logging\n",
    "        if (i % log_every == 0):\n",
    "            print(f\"\\tBatch {i} | BCE Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    metrics = eval_utils.get_metrics(torch.cat(all_labels), \n",
    "                                     torch.cat(all_preds))\n",
    "    metrics['loss'] = total_loss\n",
    "    \n",
    "    return metrics\n",
    "def test(model, test_loader, loss_fn,graph):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels, all_preds = [],[]\n",
    "    with torch.no_grad():\n",
    "        for i, batch in tqdm(enumerate(test_loader)):\n",
    "            # move batch dictionary to device\n",
    "            data_utils.batch_dict_to_device(batch, device)\n",
    "            labels, features = batch['labels'], batch['embeddings'] #changed to batch['embeddings'] from  batch['seq-var-matrix']\n",
    "\n",
    "            # compute prediction and loss\n",
    "            preds = model(features,graph.edge_index)\n",
    "            loss = loss_fn(preds, labels)\n",
    "\n",
    "            # tracking\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.flatten().detach().cpu())\n",
    "    \n",
    "    metrics = eval_utils.get_metrics(torch.cat(all_labels), \n",
    "                                     torch.cat(all_preds))\n",
    "    metrics['loss'] = total_loss\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train(model, \n",
    "          train_dataset,\n",
    "          test_dataset,\n",
    "          graph,\n",
    "          lr=1e-3, \n",
    "          n_epochs=10,\n",
    "          batch_size=256):\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        dataset = train_dataset, \n",
    "        batch_size = batch_size,\n",
    "        sampler = WeightedRandomSampler(train_dataset.weights('131338-0.0'), \n",
    "                                        num_samples = len(train_dataset)),\n",
    "        num_workers=12\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        dataset = test_dataset,\n",
    "        batch_size = batch_size,\n",
    "        num_workers=12\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    ptimizer = torch.optim.AdamW(model.parameters(), lr=hparams[\"train_params\"][\"lr\"])\n",
    "    \n",
    "    track_metrics = {'train':{i:None for i in range(n_epochs)}, \n",
    "                     'test': {i:None for i in range(n_epochs)}}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"Epoch #{epoch}:\")\n",
    "        train_metrics = train_epoch(model, \n",
    "                                    train_loader, \n",
    "                                    optimizer, \n",
    "                                    loss_fn,\n",
    "                                    graph,\n",
    "                                    log_every=10)\n",
    "        print(\"Train metrics:\")\n",
    "        eval_utils.print_metrics(train_metrics)\n",
    "        test_metrics = test(model, \n",
    "                            test_loader, \n",
    "                            loss_fn,\n",
    "                           graph)\n",
    "        print(\"Test metrics:\")\n",
    "        eval_utils.print_metrics(test_metrics)\n",
    "        \n",
    "        track_metrics['train'][epoch] = train_metrics\n",
    "        track_metrics['test'][epoch] = test_metrics\n",
    "    \n",
    "    return track_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd698e7-6617-445d-85cf-c7dbe73d5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can keep this in json and load it up\n",
    "hparams = {\n",
    "    \"model_params\":{\n",
    "        \"input_size\":1280,\n",
    "        \"hid_dim\":500,\n",
    "        \"alpha\": 0.2,\n",
    "        \"num_heads\":5,\n",
    "        \n",
    "    },\n",
    "    \"train_params\":{\n",
    "        \"bsz\":16,\n",
    "        \"num_epochs\":100,\n",
    "        \"lr\":1e-3\n",
    "        \n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "hparams[\"model_params\"][\"ngenes\"] = G.num_nodes\n",
    "\n",
    "# \n",
    "# hparams[\"model_params\"][\"input_size\"] = list(train_dataset.dataset.data['embeddings'].values())[0].shape[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89778fb6-1f2e-4cc6-9502-eafbf2e4d445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAT_Protein(\n",
       "  (input_layer): Linear(in_features=1280, out_features=500, bias=True)\n",
       "  (GATLayer1): GATv2Conv(500, 500, heads=1)\n",
       "  (output_layer1): Linear(in_features=11500, out_features=500, bias=True)\n",
       "  (output_layer): Linear(in_features=500, out_features=1, bias=True)\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GAT.GAT_Protein(**hparams[\"model_params\"])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ae052-ee70-4ad5-80e2-a97ac4d7a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = train(model, \n",
    "                      train_dataset,\n",
    "                      test_dataset, \n",
    "                      lr=hparams[\"train_params\"][\"lr\"], \n",
    "                      n_epochs=hparams[\"train_params\"][\"num_epochs\"],\n",
    "                      batch_size=hparams[\"train_params\"][\"bsz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea87daf-fad7-4fc4-8b10-692fbb72dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, axes):\n",
    "    metric_types = list(metrics['train'][0].keys())\n",
    "    for mt,ax in zip(metric_types, axes.flatten()):\n",
    "        ax.plot([x[mt] for x in model_metrics['train'].values()])\n",
    "        ax.plot([x[mt] for x in model_metrics['test'].values()])\n",
    "        ax.set_title(mt)\n",
    "    axes[0,0].legend(['train', 'test'])\n",
    "    \n",
    "fig, axes = plt.subplots(2,3,figsize=(12,6), constrained_layout=True)\n",
    "\n",
    "plot_metrics(model_metrics, axes)\n",
    "# fig.savefig('../figures/lr_initial.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
